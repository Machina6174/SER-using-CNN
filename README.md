# ðŸŽ¤ Speech Emotion Recognition using CNN

A deep learning project that classifies emotions from speech audio using a 2D Convolutional Neural Network trained on Mel-spectrograms.

---

## ðŸ‘¤ Author

| Name | Student ID |
|------|------------|
| **Daksh Gargi** | 2024B3A70888P |

---

## ðŸ“Š Model Performance

| Metric | Value |
|--------|-------|
| **Test Accuracy** | 89.93% |
| **Macro F1-Score** | 0.9023 |
| **Parameters** | 423,688 |
| **Emotions** | 8 classes |

### Per-Emotion Accuracy
| Emotion | Recall | Notes |
|---------|--------|-------|
| Neutral | 94% | âœ… |
| Calm | 97% | âœ… |
| Happy | 74% | âš ï¸ Confused with Surprised |
| Sad | 84% | âœ… |
| Angry | 92% | âœ… |
| Fearful | 95% | âœ… |
| Disgust | 92% | âœ… |
| Surprised | 95% | âœ… |

---

## ðŸš€ Quick Start

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/SER-using-CNN.git
cd SER-using-CNN
```

### 2. Create virtual environment

**macOS/Linux:**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

**Windows:**
```bash
python -m venv .venv
.venv\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Download dataset
```bash
python scripts/download_dataset.py
```

### 5. Run inference
```bash
python predict.py data/raw/Actor_01/03-01-05-01-01-01-01.wav
```

**Expected output:**
```
ðŸŽ¯ Predicted emotion: ANGRY (97.5% confidence)
```

---

## ðŸ“ Project Structure

```
SER-using-CNN/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # RAVDESS audio files (24 actors)
â”‚   â””â”€â”€ processed/              # Preprocessed spectrograms
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.keras        # Trained model weights
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_eda.ipynb           # Exploratory Data Analysis
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download_dataset.py    # Dataset downloader
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py       # Audio processing pipeline
â”‚   â”œâ”€â”€ model.py              # CNN architecture
â”‚   â””â”€â”€ train.py              # Training script
â”œâ”€â”€ predict.py                 # Inference script
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # This file
â””â”€â”€ JOURNEY.md                # Development story
```

---

## ðŸ—ï¸ Model Architecture

```
Input: (128, 130, 1) Mel-Spectrogram
    â”‚
    â”œâ”€â”€ Conv2D(32) â†’ BatchNorm â†’ ReLU â†’ MaxPool(2Ã—2)
    â”œâ”€â”€ Conv2D(64) â†’ BatchNorm â†’ ReLU â†’ MaxPool(2Ã—2)
    â”œâ”€â”€ Conv2D(128) â†’ BatchNorm â†’ ReLU â†’ MaxPool(2Ã—2)
    â”œâ”€â”€ Conv2D(256) â†’ BatchNorm â†’ ReLU â†’ MaxPool(2Ã—2)
    â”‚
    â”œâ”€â”€ Global Average Pooling
    â”œâ”€â”€ Dropout(0.6)
    â”œâ”€â”€ Dense(128, ReLU)
    â”œâ”€â”€ Dropout(0.4)
    â”‚
    â””â”€â”€ Dense(8, Softmax) â†’ Output
```

---

## ðŸ”§ Training Details

| Parameter | Value |
|-----------|-------|
| **Dataset** | RAVDESS (2,880 files) |
| **Split** | 80% train / 10% val / 10% test |
| **Augmentation** | Noise, Pitch Shift, Time Stretch |
| **Augmentation Ratio** | 3.1x (training only) |
| **Optimizer** | Adam (lr=0.001) |
| **Loss** | Sparse Categorical Crossentropy |
| **Epochs** | ~30 (early stopping) |
| **Best Epoch** | 13 |
| **Batch Size** | 32 |
| **Class Weights** | Applied for imbalance |

### Data Augmentation
- **Gaussian Noise**: Random noise injection
- **Pitch Shifting**: Â±4 semitones
- **Time Stretching**: 0.8x - 1.25x speed

---

## ðŸŽ¯ Usage Examples

### Basic Inference
```bash
python predict.py path/to/audio.wav
```

### With Verbose Output (all probabilities)
```bash
python predict.py path/to/audio.wav --verbose
```

### Custom Model Path
```bash
python predict.py audio.wav --model path/to/model.keras
```

---

## âš ï¸ Known Limitations & Challenges

### Acoustic Ambiguity
| Confusion Pair | Why It Happens | Human Accuracy |
|----------------|----------------|----------------|
| Calm â†” Sad â†” Neutral | All low-arousal, quiet, slow speech | ~70% |
| Happy â†” Surprised | Both high-energy, positive valence | ~80% |

> ðŸ’¡ These confusions are **inherent to audio-only emotion recognition** â€” even trained human annotators struggle with these distinctions.

### Model Biases
- **Gender Gap**: 6.5% accuracy difference (93.2% female vs 86.6% male)
- **Happy on Males**: Only 61% recall (vs 85% female) â€” male "happy" is acoustically less distinct

### Dataset Constraints
- **Acted Speech**: Professional actors, not spontaneous emotions
- **Single Sentence**: All samples are "Kids are talking by the door"
- **English Only**: No multilingual support
- **Clean Audio**: No background noise, music, or overlapping speech

---

## ðŸ”® Future Scope

| Enhancement | Expected Impact | Difficulty |
|-------------|-----------------|------------|
| **Attention Mechanisms** | Focus on emotional cues in speech | Medium |
| **Multi-Modal Fusion** | Combine audio + text + facial expressions | High |
| **Transfer Learning** | Use pretrained models (Wav2Vec2, HuBERT) | Medium |
| **Cross-Dataset Training** | Train on RAVDESS + CREMA-D + TESS | Medium |
| **Real-Time Streaming** | Process live audio input | Low |
| **Noise Robustness** | Handle real-world noisy environments | High |

---

## ðŸ“š Dataset

**RAVDESS** (Ryerson Audio-Visual Database of Emotional Speech and Song)
- 24 professional actors (12 male, 12 female)
- 8 emotions: Neutral, Calm, Happy, Sad, Angry, Fearful, Disgust, Surprised
- ~2,880 audio files

Source: [Zenodo](https://zenodo.org/record/1188976)

---